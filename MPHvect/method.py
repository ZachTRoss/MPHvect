# -*- coding: utf-8 -*-
"""method_new (1/7/26).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caxVyNmpEZLHUWNBm-EMaI-OYlNmX9Cs
"""

import numpy as np
import matplotlib.pyplot as plt
import itertools
from numba import njit, prange
from sklearn.neighbors import KernelDensity
import plotly.graph_objects as go
import matplotlib.colors as mcolors

#Normalize Signed Barcodes to Fit inside Unit Square, and replace infinite values with 1


def replace_inf_safe(barcode):
    vectors_copy = barcode.copy()

    for arr in vectors_copy:
      arr[np.isinf(arr)] = 1

    return vectors_copy

# Try import numba; if not present, fall back to no-jit
try:
    from numba import njit
    NUMBA_AVAILABLE = True
except Exception:
    def njit(func=None, **kwargs):
        if func is None:
            def _dec(f):
                return f
            return _dec
        return func
    NUMBA_AVAILABLE = False


# ---------------------------
# NUMBA-accelerated internals
# ---------------------------

@njit
def _my_lambda_numba(n, p, x):
    dim = p.size
    mn = x[0] - p[0]
    for i in range(1, dim):
        val = x[i] - p[i]
        if val < mn:
            mn = val
    if mn < 0.0:
        mn = 0.0
    return (2.0 ** (-n)) * mn


@njit
def _magn_numba(v):
    s = 0
    for i in range(v.size):
        s += v[i]
    return s


@njit
def _partial_kernel_numba(n, p, x):
    dim = p.size
    total = 0.0
    max_iter = 1 << dim
    for mask in range(max_iter):
        index_point = np.empty(dim)
        parity = 0
        for i in range(dim):
            bit = (mask >> i) & 1
            parity += bit
            index_point[i] = p[i] + (bit * (2.0 ** (-n)))
        sign = -1.0 if (parity % 2 == 1) else 1.0
        total += sign * _my_lambda_numba(n, index_point, x)
    return total


@njit
def _trans_vector_numba(dim):
    v = np.empty(dim)
    for i in range(dim):
        v[i] = 1.0
    return v


@njit
def _my_kernel_numba(n, p, x):
    diff = x - p
    mx = 0.0
    for i in range(diff.size):
        val = diff[i]
        if val < 0:
            val = -val
        if val > mx:
            mx = val
    if mx > 1.0:
        return 0.0

    tv = _trans_vector_numba(p.size)
    return _partial_kernel_numba(n, p - tv, x) - _partial_kernel_numba(n, p, x)


@njit
def _vectorize_fast_numba(n_list, p_list, pers_diagram, multiplicities):
    N = n_list.size
    M = pers_diagram.shape[0]
    out = np.zeros(N)

    for i in range(N):
        n = int(n_list[i])
        p = p_list[i]
        total = 0.0
        for j in range(M):
            total += multiplicities[j] * _my_kernel_numba(n, p, pers_diagram[j])
        out[i] = total

    return out


# ---------------------------
# Python fallbacks if numba is unavailable
# ---------------------------

if not NUMBA_AVAILABLE:

    def _my_lambda_numba(n, p, x):
        diff = x - p
        mn = diff[0]
        for i in range(1, diff.size):
            if diff[i] < mn:
                mn = diff[i]
        if mn < 0:
            mn = 0.0
        return (2.0 ** -n) * mn

    def _magn_numba(v):
        return int(np.sum(v))

    def _partial_kernel_numba(n, p, x):
        dim = p.size
        total = 0.0
        for mask in range(1 << dim):
            v = np.array([(mask >> i) & 1 for i in range(dim)])
            parity = np.sum(v)
            index_point = p + v * (2.0 ** (-n))
            total += ((-1)**parity) * _my_lambda_numba(n, index_point, x)
        return total

    def _trans_vector_numba(dim):
        return np.ones(dim)

    def _my_kernel_numba(n, p, x):
        if np.max(np.abs(x - p)) > 1:
            return 0.0
        return _partial_kernel_numba(n, p - _trans_vector_numba(p.size), x) - \
               _partial_kernel_numba(n, p, x)

    def _vectorize_fast_numba(n_list, p_list, pers_diagram, multiplicities):
        N = len(n_list)
        out = np.zeros(N)
        for i in range(N):
            total = 0
            for j in range(len(pers_diagram)):
                total += multiplicities[j] * _my_kernel_numba(n_list[i], p_list[i], pers_diagram[j])
            out[i] = total
        return out


# ---------------------------
# renaming with simpler function titles
# ---------------------------

def my_lambda(n, p, x):
    return _my_lambda_numba(n, p, x)

def magn(v):
    return _magn_numba(v)

def partial_kernel(n, p, x):
    return _partial_kernel_numba(n, p, x)

def trans_vector(dim_diagrams):
    return _trans_vector_numba(dim_diagrams)

def my_kernel(n, p, x):
    return _my_kernel_numba(n, p, x)


# ---------------------------
# Vertex generation (Python only)
# ---------------------------

def generate_V_n(dim, n):
    nums = range(0, 2**n + 1)
    V = []

    for point in itertools.product(nums, repeat=2 * dim):
        if all(a % 2 == 0 for a in point):
            continue
        if not all(point[i] < point[i + dim] for i in range(dim)):
            continue

        scaled = tuple(a / (2**n) for a in point)
        V.append(scaled)

    V.sort()
    return np.array(V, dtype=float)


def collect_vertices(dim, max_layer):
    all_n = []
    all_p = []
    for n in range(max_layer + 1):
        Vn = generate_V_n(dim, n)
        for p in Vn:
            all_n.append(n)
            all_p.append(p)
    return np.array(all_n, dtype=np.int64), np.array(all_p, dtype=float)


# ---------------------------
# New version of generate_vect_map returning arrays, not lambdas
# ---------------------------

def generate_vect_map(dim, max_layer):
    return collect_vertices(dim, max_layer)


# ---------------------------
# Original vectorization functions
# ---------------------------

def vectorize(index_list, pers_diagram):
    """
    index_list should be (n_list, p_list).
    """
    n_list, p_list = index_list
    out = []
    for n, p in zip(n_list, p_list):
        total = 0.0
        for x in pers_diagram:
            total += my_kernel(n, p, x)
        out.append(total)
    return np.array(out)


def vectorize_fast(n_list, p_list, pers_diagram, multiplicities):
    return _vectorize_fast_numba(n_list, p_list, pers_diagram, multiplicities)


# ---------------------------
# One-shot vectorization map
# ---------------------------

def MPHvect_vectorization_map(pers_diagram, multiplicities, dim=2, max_layer=4):
    if not np.all(np.isfinite(pers_diagram)):
        raise ValueError("pers_diagram contains non-finite values")
    if np.any(pers_diagram > 1):
        raise ValueError("pers_diagram entries must be <= 1")

    n_list, p_list = collect_vertices(dim, max_layer)
    return vectorize_fast(n_list, p_list, pers_diagram, multiplicities)

#Visualize this Vectorization on the Signed Barcode

# Number of colors
n_colors = 8

# Choose a colormap (perceptually uniform is best for distinguishing colors)
cmap = plt.get_cmap("YlGnBu")  # you can try "plasma", "cividis", etc.



# Sample the colormap at n_colors evenly spaced points
my_colors = [cmap(i / (n_colors - 1)) for i in range(n_colors)]

# Choose a colormap (perceptually uniform is best for distinguishing colors)
cmap = plt.get_cmap("magma")  # you can try "plasma", "cividis", etc.



# Sample the colormap at n_colors evenly spaced points
my_neg_colors = [cmap(i / (n_colors - 1)) for i in range(n_colors)]




# Plot the colors to visualize

# Print RGBA values for reference
#for i, color in enumerate(colors):
   # print(f"Color {i+1}: {color}")
plotly_colors = [f"rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, {a})"
                 for r, g, b, a in my_colors]

plotly_neg_colors = [f"rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, {a})"
                 for r, g, b, a in my_neg_colors]


def add_quads(fig, quads, color, opacity=0.6):
    """Batch add quads as a single Mesh3d trace (two triangles per quad)."""
    xs, ys, zs = [], [], []
    i, j, k = [], [], []

    for q_idx, corners in enumerate(quads):
        # corners = [a, b, c, d]
        a, b, c, d = corners
        base = 4 * q_idx

        xs.extend([a[0], b[0], c[0], d[0]])
        ys.extend([a[1], b[1], c[1], d[1]])
        zs.extend([a[2], b[2], c[2], d[2]])

        # two triangles: (a,b,c) and (a,c,d)
        i.extend([base, base])
        j.extend([base+1, base+2])
        k.extend([base+2, base+3])

    fig.add_trace(go.Mesh3d(
        x=xs, y=ys, z=zs,
        i=i, j=j, k=k,
        color=color, opacity=opacity,
        flatshading=True,
        showscale=False
    ))

def plot_razor_blades(bars, multiplicities, vectorizations, colors=plotly_colors, neg_colors=plotly_neg_colors):


    fig = go.Figure()

    # Base XY-plane shading
    grid_x = np.linspace(0, 1, 100)
    grid_y = np.linspace(0, 1.25, 100)
    Zg = np.zeros((100,100))
    fig.add_trace(go.Surface(
        x=grid_x, y=grid_y, z=Zg,
        opacity=0.5, showscale=False,
        colorscale="Blues"
    ))

    # Collect quads grouped by color
    pos_quads = {c: [] for c in colors}
    neg_quads = {c: [] for c in neg_colors}

    for i, x in enumerate(bars):
        mult = multiplicities[i]
        vector = vectorizations[i]

        # Base line (red)
        fig.add_trace(go.Scatter3d(
            x=[x[0], x[2]], y=[x[1], x[3]], z=[0,0],
            mode='lines',
            line=dict(color='red', width=3),
            showlegend=False
        ))

        z_start = 0
        for j, length in enumerate(vector):
            z_end = z_start + length
            corners = np.array([
                [x[0], x[1], z_start],
                [x[2], x[3], z_start],
                [x[2], x[3], z_end],
                [x[0], x[1], z_end]
            ])
            if mult >=0:
                pos_quads[colors[j % len(colors)]].append(corners)
            elif mult <=0:
                neg_quads[neg_colors[j % len(neg_colors)]].append(corners)
            z_start = z_end

    # Batch add quads
    for color, quads in pos_quads.items():
        if quads:
            add_quads(fig, quads, color, opacity=0.6)
    for color, quads in neg_quads.items():
        if quads:
            add_quads(fig, quads, color, opacity=0.6)

    # Layout once at the end
    fig.update_layout(
        scene=dict(
            xaxis=dict(range=[0, 0.5], showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(range=[0, 1], showgrid=False, zeroline=False, showticklabels=False),
            zaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        ),
        width=700, height=600,
        margin=dict(l=0, r=0, t=0, b=0),
        showlegend=False
    )

    fig.show()

#This function plots a visualization of the vectorization of a 1-parameter persistence module.
def plot_nailbed(points, vectors, colors=plotly_colors):


    fig = go.Figure()
    vectors=np.array(vectors)
    y_max = max(points[:,1])
    x_min = min(points[:,0])

    # Shaded region x <= y (in XY-plane at z=0)
    grid_x = np.linspace(0-0.25*y_max, y_max, 200)
    grid_y = np.linspace(0-0.25*y_max, y_max, 200)
    Xg, Yg = np.meshgrid(grid_x, grid_y)
    mask = Xg <= Yg
    Zg = np.zeros_like(Xg)

    Xmask = np.where(mask, Xg, np.nan)
    Ymask = np.where(mask, Yg, np.nan)
    Zmask = np.where(mask, Zg, np.nan)

    fig.add_trace(go.Surface(
        x=Xmask, y=Ymask, z=Zmask,
        opacity=0.5, showscale=False,
        colorscale="Blues"
    ))

    # Black boundary line (x=y, z=0)
    line_range = np.linspace(-2, 4, 100)
    fig.add_trace(go.Scatter3d(
        x=line_range, y=line_range, z=np.zeros_like(line_range),
        mode="lines", line=dict(color="black", width=6)
    ))

    # Grouped line segments for stacked vectors
    for j in range(vectors.shape[1]):  # each "layer" index
        xs, ys, zs = [], [], []
        for i, x in enumerate(points):
            Y = vectors[i]
            # compute segment start/end for this layer
            z_start = sum(Y[:j]) if j < len(Y) else None
            z_end = z_start + Y[j] if j < len(Y) else None
            if z_start is not None:
                xs.extend([x[0], x[0], None])
                ys.extend([x[1], x[1], None])
                zs.extend([z_start, z_end, None])

        fig.add_trace(go.Scatter3d(
            x=xs, y=ys, z=zs,
            mode="lines",
            line=dict(color=colors[j % len(colors)], width=6),
            showlegend=False
        ))

    # Red base points
    fig.add_trace(go.Scatter3d(
        x=points[:,0], y=points[:,1], z=np.zeros(points.shape[0]),
        mode="markers", marker=dict(size=3, color="red"),
        name="base points"
    ))

    # Layout cleanup
    fig.update_layout(
        scene=dict(
            xaxis=dict(range=[0.9*x_min, 1*y_max],
                       showgrid=False, zeroline=False, showticklabels=False, showbackground=False, title=''),
            yaxis=dict(range=[0.9*x_min, 1.1*y_max],
                       showgrid=False, zeroline=False, showticklabels=False, showbackground=False, title=''),
            zaxis=dict(range=[0, np.max(vectors.sum(axis=1))+1],
                       showgrid=False, zeroline=False, showticklabels=False, showbackground=False, title=''),
        ),
        width=700, height=600,
        margin=dict(l=0, r=0, t=0, b=0),
        showlegend=False
    )

    fig.show()

# This function is a wrap up of the previous two functions. It is the most user-friendly. You can use this to takeinputs of a persistence diagram (and possibly multiplicities), as well as the number of triangulations you wish to use in your vectorizatoin, and output the visualization for the vectorization of given diagram, whether it's 1-parameter or 2-parameter.  Note: This normalizes persistence diagrams by (10*max entry /9), then replaces infinite values with 1. This way, infinite lifespans still rank more highly than the maximal finite lifespan(s).

def MPHvect_visualize(points, multiplicities=None, number_of_triangulations=5):

  if points[0].size==2:
    vecmap=generate_vect_map(1,number_of_triangulations)
    max_val = max(arr[np.isfinite(arr)].max() for arr in points)
    norm_pd = 0.9*points / max_val

    normalized_diagram=replace_inf_safe(norm_pd)

    list_of_vecs=[]
    for x in normalized_diagram:
      list_of_vecs.append(vectorize(vecmap, np.array([x])))

    plot_nailbed(normalized_diagram, list_of_vecs, plotly_colors)

  elif points[0].size==4:
      if multiplicities is None:
        raise ValueError("If points are 4D, multiplicities must be provided as an np.array of equal length to 'points'.")

      vecmap=generate_vect_map(2,number_of_triangulations)
      max_val = max(arr[np.isfinite(arr)].max() for arr in points)
      norm_pd = 0.9*points / max_val

      normalized_diagram=replace_inf_safe(norm_pd)

      list_of_vecs=[]
      for i,x in enumerate(normalized_diagram):
        list_of_vecs.append(multiplicities[i]*vectorize(vecmap, np.array([x])))
      plot_razor_blades(normalized_diagram, multiplicities, list_of_vecs, colors=plotly_colors, neg_colors=plotly_neg_colors)
  else:
      raise ValueError("Persistent Diagram must consist of points in 2D or 4D")

from sklearn import svm
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#define SVM+PCA as a single function to make things easier when doing classifications in the examples
def do_SVM_and_PCA(class1, class2):

  # Combine data and create labels
  X = np.vstack((class1, class2))
  y = np.array([0]*len(class1) + [1]*len(class2))

  # Split into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

  # Train SVM
  clf = SVC(kernel='linear')
  clf.fit(X_train, y_train)

  # Predict and calculate accuracy
  y_pred = clf.predict(X_test)
  observed_accuracy = accuracy_score(y_test, y_pred)
  print("SVM Accuracy:", observed_accuracy)

  # ==========================
  # Permutation test for p-value
  # ==========================
  permuted_accuracies = []

  for _ in range(n_permutations):
      y_perm = np.random.permutation(y)

      X_tr, X_te, y_tr, y_te = train_test_split(
          X, y_perm, test_size=0.2, stratify=y_perm
      )

      clf_perm = SVC(kernel='linear')
      clf_perm.fit(X_tr, y_tr)
      y_perm_pred = clf_perm.predict(X_te)

      permuted_accuracies.append(
          accuracy_score(y_te, y_perm_pred)
      )

  permuted_accuracies = np.array(permuted_accuracies)

  p_value = (np.sum(permuted_accuracies >= observed_accuracy) + 1) / (n_permutations + 1)
  print("Permutation-test p-value:", p_value)

  # PCA to 3D
  pca = PCA(n_components=3)
  X_pca = pca.fit_transform(X)

  # Get PCA-transformed separating hyperplane
  w = clf.coef_[0]
  b = clf.intercept_[0]

  # Project the hyperplane into PCA space
  w_pca = pca.transform([w])[0] - pca.transform([[0]*X.shape[1]])[0]
  b_pca = b  # intercept remains scalar

  # Plotting
  fig = plt.figure(figsize=(10, 7))
  ax = fig.add_subplot(111, projection='3d')

  # Plot class points
  ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], X_pca[y==0, 2], color='blue', label='Class 0')
  ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], X_pca[y==1, 2], color='red', label='Class 1')

  # Create grid for hyperplane
  #xx, yy = np.meshgrid(np.linspace(X_pca[:, 0].min(), X_pca[:, 0].max(), 10),
                      #np.linspace(X_pca[:, 1].min(), X_pca[:, 1].max(), 10))
  #zz = (-w_pca[0]*xx - w_pca[1]*yy - b_pca) / w_pca[2]

  # Plot hyperplane
  #ax.plot_surface(xx, yy, zz, alpha=0.3, color='green', label='Hyperplane')

  s=max(max(X_pca[y==0,0]), -min(X_pca[y==0,0]), max(X_pca[y==1,0]), -min(X_pca[y==1,0]))
  t=max(max(X_pca[y==0,1]), -min(X_pca[y==0,1]), max(X_pca[y==1,1]), -min(X_pca[y==1,1]))
  u=max(max(X_pca[y==0,2]), -min(X_pca[y==0,2]), max(X_pca[y==1,2]), -min(X_pca[y==1,2]))

  ax.set_xlabel('PC1')
  ax.set_ylabel('PC2')
  ax.set_zlabel('PC3')
  ax.set_xlim(-s, s)
  ax.set_ylim(-t,t)
  ax.set_zlim(-u,u)
  ax.set_title('3D PCA Projection with SVM Hyperplane')
  ax.legend()
  plt.show()
